VERBOSE: False
BASE_TASK_CONFIG_PATH: configs/tasks/rearrange/pick.yaml
TRAINER_NAME: "ddppo"
SIMULATOR_GPU_ID: 0
TORCH_GPU_ID: 0
VIDEO_OPTION: []
# VIDEO_OPTION: ["disk"] #
TENSORBOARD_DIR: "tb"
VIDEO_DIR: "video_dir/npnp_25k_train"
VIDEO_FPS: 30
VIDEO_RENDER_TOP_DOWN: False
VIDEO_RENDER_ALL_INFO: True
VIDEO_RENDER_VIEWS:
  - "THIRD_RGB_SENSOR"
TEST_EPISODE_COUNT: -1
EVAL_CKPT_PATH_DIR: "data/ckpts/npnp_online_contd_3"
NUM_ENVIRONMENTS: 32
WRITER_TYPE: 'wb'
# Visual sensors to include
SENSORS: ["HEAD_DEPTH_SENSOR"]
CHECKPOINT_FOLDER: "data/ckpts/npnp_online_contd_3"
NUM_UPDATES: -1
TOTAL_NUM_STEPS: 1.0e8
LOG_INTERVAL: 10
NUM_CHECKPOINTS: 50
# Force PyTorch to be single threaded as
# this improves performance considerably
FORCE_TORCH_SINGLE_THREADED: True
EVAL_KEYS_TO_INCLUDE_IN_NAME: ['success', 'reward']

RL:
  POLICY:
      name: "TransformerResNetPolicy"
      action_distribution_type: "mixed"
      ACTION_DIST:
        discrete_arm: true
        discrete_base: false
        use_std_param: true
      # action_distribution_type: "gaussian"
      # ACTION_DIST:
        log_std_init: -1.0
      use_rgb: false
      offline: false
      include_visual_keys: ["robot_head_depth"] # "robot_head_depth", robot_head_rgb

  GYM_OBS_KEYS: ['obj_start_sensor', 'obj_start_gps_compass', 'obj_goal_sensor', 'obj_goal_gps_compass', 'relative_resting_position', 'joint', 'is_holding']

  TRANSFORMER:
    return_to_go: 100
    context_length: 30 #10
    model_type: "bc"
    n_head: 8
    n_layer: 6
    freeze_layer: [0,1,2,3] #
    reg_flags: 
      outer_dropout: false
      outer_layernorm: true
      attention_dropout: false
      attention_layernorm: true
      feedforward_dropout: false
      feedforward_layernorm: true
    hidden_size: 512
    backbone: resnet18

  PPO:
    # ppo params
    clip_param: 0.1
    ppo_epoch: 2
    num_mini_batch: 2
    value_loss_coef: .5
    entropy_coef: 0.000 # 0.0005
    lr: 1e-6
    eps: 1e-5
    max_grad_norm: 0.1
    num_steps: 128
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: True
    use_warmup_linear_lr_decay: True
    warmup_updates: 1
    reward_window_size: 50

    value_func_warmup: 200

    use_normalized_advantage: True

    hidden_size: 512
    
    # Use double buffered sampling, typically helps
    # when environment time is similar or large than
    # policy inference time during rollout generation
    use_double_buffered_sampler: False

  DDPPO:
    sync_frac: 0.6
    # The PyTorch distributed backend to use
    distrib_backend: NCCL
    # Visual encoder backbone
    pretrained_weights: "data/ckpts/npnp_25k_offline_embd_obsatt_2/ckpt.22.pth" #
    # pretrained_weights: "data/ckpts/npnp_online_pretrained_2/ckpt.8.pth" #
    # pretrained_weights: "data/ckpts/npnp_online_scratch_2/ckpt.7.pth" #
    # Initialize with pretrained weights
    pretrained: True
    # Initialize just the visual encoder backbone with pretrained weights
    pretrained_encoder: False
    # Whether or not the visual encoder backbone will be trained.
    train_encoder: False
    # Whether or not to reset the critic linear layer
    reset_critic: False

    # Model parameters
    backbone: resnet18
    rnn_type: LSTM
    num_recurrent_layers: 2

  

WB:
  PROJECT_NAME: "p-skill-transformer"
  ENTITY: "cvmlp-whoknowsss"
  RUN_NAME: "lr=1e-6-ref-2"
  GROUP: "trfm_pretrained_new"

